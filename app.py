import os
import json
import tempfile
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from datetime import datetime

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, silhouette_score

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

try:
    import google.generativeai as genai
    GENAI_ENABLED = True
except ImportError:
    GENAI_ENABLED = False

try:
    from scapy.all import rdpcap, IP, TCP, UDP
    SCAPY_AVAILABLE = True
except ImportError:
    SCAPY_AVAILABLE = False

st.set_page_config(page_title="üöÄ Deep Cybersecurity ML Analyzer with Gemini", layout="wide")

@st.cache_data(show_spinner=False)
def load_csv(file):
    try:
        return pd.read_csv(file)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
        return pd.DataFrame()

@st.cache_data(show_spinner=False)
def process_pcap(file):
    if not SCAPY_AVAILABLE:
        st.error("Scapy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–µ—Ä–µ–∑ `pip install scapy`.")
        return pd.DataFrame()
    content = file.read()
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pcap") as tmp:
        tmp.write(content)
        tmp_path = tmp.name
    try:
        packets = rdpcap(tmp_path)
    except Exception as e:
        os.unlink(tmp_path)
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PCAP: {e}")
        return pd.DataFrame()
    os.unlink(tmp_path)

    records = []
    for i, pkt in enumerate(packets):
        if IP in pkt:
            rec = {
                "packet_id": i,
                "src_ip": pkt[IP].src,
                "dst_ip": pkt[IP].dst,
                "protocol": pkt[IP].proto,
                "length": len(pkt),
                "ttl": pkt[IP].ttl,
                "timestamp": pkt.time if hasattr(pkt, 'time') else np.nan,
            }
            if TCP in pkt:
                rec.update({
                    "src_port": pkt[TCP].sport,
                    "dst_port": pkt[TCP].dport,
                    "tcp_flags": int(pkt[TCP].flags),
                    "window_size": pkt[TCP].window,
                })
            elif UDP in pkt:
                rec.update({
                    "src_port": pkt[UDP].sport,
                    "dst_port": pkt[UDP].dport,
                    "udp_length": pkt[UDP].len,
                })
            records.append(rec)
    df = pd.DataFrame(records)
    if not df.empty:
        df["flow_id"] = df.apply(
            lambda x: f"{x['src_ip']}:{x.get('src_port',0)}->{x['dst_ip']}:{x.get('dst_port',0)}", axis=1)
        # –î–æ–±–∞–≤–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ä–∞–º–∫–∞—Ö –ø–æ—Ç–æ–∫–∞:
        df = df.groupby("flow_id").agg({
            "length": ["mean", "sum", "count"],
            "ttl": ["mean", "min", "max"],
            "timestamp": ["min", "max", "std"],
            "tcp_flags": "mean",
            "window_size": "mean",
            "udp_length": "mean"
        }).reset_index()
        df.columns = ["_".join(col).strip("_") for col in df.columns.values]
        df["flow_duration"] = df["timestamp_max"] - df["timestamp_min"]
        df["flow_timestamp_std"] = df["timestamp_std"].fillna(0)
    return df

def preprocess_features(df, feature_cols):
    X = df[feature_cols].copy()
    for col in X.columns:
        if X[col].isnull().any():
            if pd.api.types.is_numeric_dtype(X[col]):
                X[col].fillna(X[col].median(), inplace=True)
            else:
                X[col].fillna(X[col].mode()[0], inplace=True)
    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    for col in cat_cols:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    return X_scaled, X.columns.tolist()

def generate_analysis_summary(atype, params, metrics):
    lines = [f"–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {atype}", f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}", "–ú–µ—Ç—Ä–∏–∫–∏:"]
    for k,v in metrics.items():
        lines.append(f"  {k}: {v}")
    return "\n".join(lines)

def call_gemini_with_retries(prompt, max_retries=3):
    if not GENAI_ENABLED:
        st.warning("Gemini API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return {}
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    model = genai.GenerativeModel("gemini-pro")
    for i in range(max_retries):
        try:
            response = model.generate_content(prompt, timeout=15)
            j = json.loads(response.text)
            if j:
                return j
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ Gemini, –ø–æ–ø—ã—Ç–∫–∞ {i+1}: {e}")
    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Gemini –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫")
    return {}

def plot_interactive_pca(X_scaled, labels, title):
    pca = PCA(n_components=2)
    pcs = pca.fit_transform(X_scaled)
    df_plot = pd.DataFrame({'PC1': pcs[:,0], 'PC2': pcs[:,1], 'Label': labels.astype(str)})
    fig = px.scatter(df_plot, x='PC1', y='PC2', color='Label', title=title, labels={'PC1':'PC1','PC2':'PC2'}, hover_data=df_plot.columns)
    st.plotly_chart(fig, use_container_width=True)

def explain_classification_model(clf, X_train, feature_names):
    if not SHAP_AVAILABLE:
        st.warning("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ SHAP –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: pip install shap")
        return
    st.subheader("üß© –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å SHAP")
    explainer = shap.TreeExplainer(clf)
    shap_values = explainer.shap_values(X_train)
    shap.summary_plot(shap_values, features=X_train, feature_names=feature_names, show=False)
    st.pyplot(bbox_inches='tight')

def analyze_tcp_flags(df):
    st.subheader("üîç –ê–Ω–∞–ª–∏–∑ TCP-—Ñ–ª–∞–≥–æ–≤")
    if "tcp_flags_mean" not in df.columns:
        st.write("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö TCP —Ñ–ª–∞–≥–æ–≤")
        return
    df["syn_flag"] = df["tcp_flags_mean"].apply(lambda x: (int(x) & 0x02) > 0)
    syn_count = df["syn_flag"].sum()
    st.write(f"–ü–æ—Ç–æ–∫–æ–≤ —Å SYN-—Ñ–ª–∞–≥–æ–º: {syn_count} –∏–∑ {len(df)}")
    st.dataframe(df[["flow_id", "tcp_flags_mean", "syn_flag"]].head(10))

def behavioral_analysis(df):
    st.subheader("üìà –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–æ–≤")
    if "flow_duration" in df.columns:
        fig = px.histogram(df, x="flow_duration", nbins=30, title="–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Ç–æ–∫–æ–≤ (—Å–µ–∫—É–Ω–¥—ã)")
        st.plotly_chart(fig, use_container_width=True)
    if "length_sum" in df.columns and "length_count" in df.columns:
        df["avg_packet_size"] = df["length_sum"] / df["length_count"]
        fig2 = px.histogram(df, x="avg_packet_size", nbins=30, title="–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –≤ –ø–æ—Ç–æ–∫–µ")
        st.plotly_chart(fig2, use_container_width=True)

def kb_report(df):
    st.subheader("üõ°Ô∏è –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
    if "flow_duration" in df.columns and "length_count" in df.columns:
        suspect = df[(df["flow_duration"] < 0.1) & (df["length_count"] > df["length_count"].median())]
        st.write(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: {len(suspect)}")
        st.dataframe(suspect.head(10))
    else:
        st.warning("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ 'flow_duration' –∏/–∏–ª–∏ 'length_count' –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤.")
        st.write("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:", df.columns.tolist())

# --- –û—Å–Ω–æ–≤–Ω–æ–π UI ---

st.title("üöÄ –ì–ª—É–±–æ–∫–∏–π –ö–ë –∏ ML –∞–Ω–∞–ª–∏–∑ —Å–µ—Ç–µ–≤–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ —Å Gemini")

file_type = st.sidebar.selectbox("–¢–∏–ø —Ñ–∞–π–ª–∞", ["CSV", "PCAP"])
uploaded_file = st.sidebar.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª", type=["csv"] if file_type=="CSV" else ["pcap", "pcapng"])

if uploaded_file:
    df = load_csv(uploaded_file) if file_type=="CSV" else process_pcap(uploaded_file)
    if df.empty:
        st.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        st.stop()

    st.subheader("–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö")
    st.dataframe(df.head())

    id_col = st.sidebar.selectbox("ID –∫–æ–ª–æ–Ω–∫–∞", options=df.columns, index=0)
    default_feats = df.select_dtypes(include=np.number).columns.tolist()
    if id_col in default_feats:
        default_feats.remove(id_col)
    feat_cols = st.sidebar.multiselect("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", options=[c for c in df.columns if c!=id_col], default=default_feats[:5])

    if len(feat_cols) == 0:
        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫")
        st.stop()

    analysis_type = st.sidebar.radio("–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞", ["Clustering", "Classification", "Anomaly Detection"])

    if "ml_params" not in st.session_state:
        st.session_state.ml_params = {"n_clusters": 3, "contamination": 0.1, "n_estimators": 100}

    st.sidebar.header("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
    if analysis_type == "Clustering":
        st.session_state.ml_params["n_clusters"] = st.sidebar.slider("–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 2, 15, st.session_state.ml_params["n_clusters"])
    elif analysis_type == "Anomaly Detection":
        st.session_state.ml_params["contamination"] = st.sidebar.slider("–î–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π", 0.01, 0.5, st.session_state.ml_params["contamination"], 0.01)
    elif analysis_type == "Classification":
        st.session_state.ml_params["n_estimators"] = st.sidebar.slider("–ß–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤", 10, 300, st.session_state.ml_params["n_estimators"], 10)

    # –ö–ë –∞–Ω–∞–ª–∏–∑
    analyze_tcp_flags(df)
    behavioral_analysis(df)
    kb_report(df)

    X_scaled, feature_names = preprocess_features(df, feat_cols)

    if st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å ML-–∞–Ω–∞–ª–∏–∑"):
        if analysis_type == "Clustering":
            model = KMeans(n_clusters=st.session_state.ml_params["n_clusters"], random_state=42)
            labels = model.fit_predict(X_scaled)
            df['Cluster'] = labels
            sil_score = silhouette_score(X_scaled, labels)
            st.write(f"Silhouette score: {sil_score:.4f}")
            st.bar_chart(df['Cluster'].value_counts())
            plot_interactive_pca(X_scaled, labels, f"KMeans Clustering (k={st.session_state.ml_params['n_clusters']}) PCA")

            summary = generate_analysis_summary(analysis_type, st.session_state.ml_params, {"silhouette_score": sil_score})

        elif analysis_type == "Classification":
            target_cols = [c for c in df.columns if c not in feat_cols + [id_col]]
            if not target_cols:
                st.error("–ù–µ—Ç —Ü–µ–ª–µ–≤–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                st.stop()
            target_col = st.selectbox("–¶–µ–ª–µ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü", target_cols)
            y = df[target_col]

            X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
            clf = RandomForestClassifier(n_estimators=st.session_state.ml_params["n_estimators"], random_state=42)
            clf.fit(X_train, y_train)

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
            scores = cross_val_score(clf, X_scaled, y, cv=5)
            st.write(f"Cross-Validation Accuracy: {np.mean(scores):.4f} ¬± {np.std(scores):.4f}")

            y_pred = clf.predict(X_test)
            report = classification_report(y_test, y_pred, output_dict=True)
            st.subheader("–û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            st.dataframe(pd.DataFrame(report).transpose())
            imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': clf.feature_importances_}).sort_values('Importance', ascending=False)
            fig = px.bar(imp_df, x='Importance', y='Feature', orientation='h', title="–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            st.plotly_chart(fig, use_container_width=True)

            pred_labels = clf.predict(X_scaled)
            plot_interactive_pca(X_scaled, pred_labels, "Random Forest Classification PCA")

            # SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
            explain_classification_model(clf, X_train, feature_names)

            summary = generate_analysis_summary(analysis_type, st.session_state.ml_params, {"accuracy": report.get("accuracy", "N/A")})

        else:  # Anomaly Detection
            model = IsolationForest(contamination=st.session_state.ml_params["contamination"], random_state=42)
            preds = model.fit_predict(X_scaled)
            df['Anomaly'] = (preds == -1).astype(int)
            count_anom = df['Anomaly'].sum()
            total = len(df)
            anomaly_ratio = count_anom / total
            st.write(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {count_anom} –∏–∑ {total} ({anomaly_ratio:.2%})")
            st.dataframe(df[df['Anomaly'] == 1].head(20))
            plot_interactive_pca(X_scaled, df['Anomaly'], "Isolation Forest Anomaly Detection PCA")

            summary = generate_analysis_summary(analysis_type, st.session_state.ml_params, {
                "anomalies_detected": count_anom,
                "anomaly_ratio": f"{anomaly_ratio:.2%}"
            })

        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã ML-–∞–Ω–∞–ª–∏–∑–∞")
        st.code(summary)

        if GENAI_ENABLED:
            st.subheader("–ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç Gemini...")
            new_params = call_gemini_with_retries(f"""
You are an expert ML engineer and cybersecurity analyst. Based on this analysis summary, recommend improved ML parameters as JSON:

{summary}

Return JSON with keys such as "n_clusters", "contamination", "n_estimators".
""")
            if new_params:
                st.success("Gemini —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
                st.json(new_params)

                n_clusters = int(new_params.get("n_clusters", st.session_state.ml_params["n_clusters"]))
                if n_clusters < 2 or n_clusters > 15:
                    n_clusters = st.session_state.ml_params["n_clusters"]
                contamination = float(new_params.get("contamination", st.session_state.ml_params["contamination"]))
                if contamination < 0.01 or contamination > 0.5:
                    contamination = st.session_state.ml_params["contamination"]
                n_estimators = int(new_params.get("n_estimators", st.session_state.ml_params["n_estimators"]))
                if n_estimators < 10 or n_estimators > 300:
                    n_estimators = st.session_state.ml_params["n_estimators"]

                st.session_state.ml_params.update({
                    "n_clusters": n_clusters,
                    "contamination": contamination,
                    "n_estimators": n_estimators
                })

                st.info("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –∑–∞–Ω–æ–≤–æ —Å –Ω–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.")
            else:
                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç Gemini.")
else:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ PCAP —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.")

st.markdown("---")
st.caption("Powered by Streamlit, scikit-learn, SHAP & Google Gemini API")
